{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "from bs4 import BeautifulSoup \n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read DF\n",
    "df = pd.read_csv('200Reviews.csv')\n",
    "raw_reviews = df['review'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization, lemmatization, stopword removal\n",
    "stopWords = set(stopwords.words('english'))\n",
    "WNL = WordNetLemmatizer()\n",
    "\n",
    "def replacer(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" is \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "reviews = []\n",
    "\n",
    "for i in range(len(raw_reviews)):\n",
    "    sentence = BeautifulSoup(raw_reviews[i]).get_text()\n",
    "    sentence = replacer(sentence)\n",
    "    sentence = re.sub(r'[^\\w\\s]', ' ', sentence)\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    clean_sentence = ''\n",
    "    for w in words:\n",
    "        w = w.lower()\n",
    "        w = WNL.lemmatize(w)\n",
    "        if w not in stopWords:\n",
    "            clean_sentence += w + ' '\n",
    "    reviews.append(clean_sentence.rstrip(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Co-occurrence matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>000</th>\n",
       "      <th>1</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>11</th>\n",
       "      <th>117</th>\n",
       "      <th>12</th>\n",
       "      <th>13th</th>\n",
       "      <th>...</th>\n",
       "      <th>zhou</th>\n",
       "      <th>zigfield</th>\n",
       "      <th>zion</th>\n",
       "      <th>zip</th>\n",
       "      <th>zombi</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zuniga</th>\n",
       "      <th>êxtase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zombie</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zone</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zoom</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zuniga</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>êxtase</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6556 rows × 6556 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0  000  1  10  100  101  11  117  12  13th  ...  zhou  zigfield  zion  \\\n",
       "0       0    0  0   2    0    0   0    0   0     0  ...     0         0     0   \n",
       "000     0    0  0   1    0    0   1    0   0     0  ...     0         0     0   \n",
       "1       0    0  0   2    0    0   1    0   0     0  ...     0         0     0   \n",
       "10      2    1  2   3    0    0   0    0   0     0  ...     0         0     0   \n",
       "100     0    0  0   0    0    0   0    0   0     0  ...     0         0     0   \n",
       "...    ..  ... ..  ..  ...  ...  ..  ...  ..   ...  ...   ...       ...   ...   \n",
       "zombie  0    0  4   0    0    0   0    0   0     0  ...     0         0     0   \n",
       "zone    0    0  0   0    0    0   0    0   0     0  ...     0         0     0   \n",
       "zoom    0    0  0   0    0    0   0    0   0     0  ...     0         0     0   \n",
       "zuniga  0    0  0   0    0    0   0    0   0     0  ...     0         0     0   \n",
       "êxtase  0    0  0   0    0    0   0    0   0     0  ...     0         0     0   \n",
       "\n",
       "        zip  zombi  zombie  zone  zoom  zuniga  êxtase  \n",
       "0         0      0       0     0     0       0       0  \n",
       "000       0      0       0     0     0       0       0  \n",
       "1         0      0       4     0     0       0       0  \n",
       "10        0      0       0     0     0       0       0  \n",
       "100       0      0       0     0     0       0       0  \n",
       "...     ...    ...     ...   ...   ...     ...     ...  \n",
       "zombie    0      2      13     0     0       0       0  \n",
       "zone      0      0       0     0     0       0       0  \n",
       "zoom      0      0       0     0     0       0       0  \n",
       "zuniga    0      0       0     0     0       0       0  \n",
       "êxtase    0      0       0     0     0       0       0  \n",
       "\n",
       "[6556 rows x 6556 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create co-occurrence matrix\n",
    "window_size = 5\n",
    "\n",
    "def co_occurrence(sentences, window_size):\n",
    "    d = defaultdict(int)\n",
    "    vocab = set()\n",
    "    for text in sentences:\n",
    "        text = text.split()\n",
    "        # iterate over sentences\n",
    "        for i in range(len(text)):\n",
    "            token = text[i]\n",
    "            vocab.add(token)  # add to vocab\n",
    "            next_token = text[i+1 : i+1+window_size]\n",
    "            for t in next_token:\n",
    "                key = tuple( sorted([t, token]) )\n",
    "                d[key] += 1\n",
    "\n",
    "    # formulate the dictionary into dataframe\n",
    "    vocab = sorted(vocab) # sort vocab\n",
    "    df = pd.DataFrame(data=np.zeros((len(vocab), len(vocab)), dtype=np.int16),\n",
    "                      index=vocab,\n",
    "                      columns=vocab)\n",
    "    \n",
    "    for key, value in d.items():\n",
    "        df.at[key[0], key[1]] = value\n",
    "        df.at[key[1], key[0]] = value\n",
    "        \n",
    "    return df\n",
    "\n",
    "df = co_occurrence(reviews, window_size)\n",
    "df_array = np.array(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create reference dictionaries\n",
    "words = df.index.tolist()\n",
    "word_to_index = {}\n",
    "index_to_word = {}\n",
    "\n",
    "for i in range(len(words)):\n",
    "    word_to_index[words[i]] = i\n",
    "    index_to_word[i] = words[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I fed the co-occurrence matrix into an SVD to output dense word vectors of length 100. I also created some dictionaries to be used in helping me map words to index to word embeddings, these will be used later on to compare the similarity between words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6556, 100)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SVD\n",
    "word_embedding_size = 100\n",
    "\n",
    "svd = TruncatedSVD(n_components = word_embedding_size, n_iter = 7, random_state = 42)\n",
    "svd_output = svd.fit_transform(df_array)\n",
    "svd_output.shape # 6556 vectors of dimension 100 each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_embedding = {} # Another reference dictionary\n",
    "for i in range(svd_output.shape[0]):\n",
    "    index_to_embedding[i] = svd_output[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I created a function svd_most_similar(word) that takes in a word and outputs the top 10 similar words found. Since word2vec uses cosine similarity in its most_similar function, I opted to use cosine similarity as well for consistency sake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word similarity function similar to word2vec's most_similar function\n",
    "def svd_most_similar(word):\n",
    "    idx = word_to_index[word]\n",
    "    embedding = index_to_embedding[idx]\n",
    "    diff_dict = {}\n",
    "    \n",
    "    x = cosine_similarity(np.array(embedding.tolist()).reshape((1, 100)), svd_output)\n",
    "    for i in range(len(x[0])):\n",
    "        diff_dict[i] = x[0][i]\n",
    "\n",
    "    sorted_diff_dict = {k: v for k, v in sorted(diff_dict.items(), key=lambda item: item[1])}\n",
    "\n",
    "    top_10_words = list(sorted_diff_dict.keys())[-11:len(sorted_diff_dict)-1][::-1]\n",
    "    top_10_values = list(sorted_diff_dict.values())[-11:len(sorted_diff_dict)-1][::-1]\n",
    "    \n",
    "    output = []\n",
    "    for x, y in zip(top_10_words, top_10_values):\n",
    "        output.append((index_to_word[x], y))\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert input to list of lists before input into word2vec model\n",
    "reviews_word2vec = []\n",
    "for sentence in reviews:\n",
    "    new_sentence = []\n",
    "    for word in sentence.split():\n",
    "        new_sentence.append(word)\n",
    "    reviews_word2vec.append(new_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the model and setting values for the various parameters\n",
    "num_features = 100  # Word vector dimensionality\n",
    "min_word_count = 1 # Minimum word count\n",
    "num_workers = 4     # Number of parallel threads\n",
    "downsampling = 1e-3 # (0.001) Downsample setting for frequent words\n",
    "\n",
    "# Initializing the train model\n",
    "from gensim.models import word2vec\n",
    "model = word2vec.Word2Vec(reviews_word2vec,\n",
    "                          workers=num_workers,\n",
    "                          size=num_features,\n",
    "                          min_count=min_word_count,\n",
    "                          sample=downsampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compare outputs of the 2 models\n",
    "def compare_models(word):\n",
    "    print('Input word: ' + word + '\\n')\n",
    "    \n",
    "    print('***** SVD *****')\n",
    "    for i in svd_most_similar(word):\n",
    "        print(i[0], i[1])\n",
    "        \n",
    "    print('\\n***** Word2Vec *****')\n",
    "    for i in model.wv.most_similar(word):\n",
    "        print(i[0], i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input word: zombie\n",
      "\n",
      "***** SVD *****\n",
      "cannibal 0.7456572548014732\n",
      "preferably 0.7293954645808435\n",
      "gruesome 0.7191783274525281\n",
      "killing 0.7073168188135955\n",
      "attack 0.7046966135758148\n",
      "bird 0.7016420484746623\n",
      "canister 0.6840418880685827\n",
      "dough 0.6690725971780732\n",
      "raking 0.6681803862492934\n",
      "assed 0.6643085694074632\n",
      "\n",
      "***** Word2Vec *****\n",
      "movie 0.946296751499176\n",
      "wa 0.9317489862442017\n",
      "film 0.9287232160568237\n",
      "bad 0.9258207082748413\n",
      "time 0.9248310327529907\n",
      "character 0.9244245886802673\n",
      "could 0.9228945970535278\n",
      "ha 0.9227113127708435\n",
      "get 0.9217485785484314\n",
      "even 0.9197666049003601\n"
     ]
    }
   ],
   "source": [
    "compare_models('zombie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input word: pleasant\n",
      "\n",
      "***** SVD *****\n",
      "activity 0.6775511592106531\n",
      "delightful 0.6649102371021132\n",
      "help 0.6494918047517543\n",
      "unifying 0.6293391662251103\n",
      "heartstrings 0.6292837517127319\n",
      "cry 0.6280458558038033\n",
      "simple 0.6258560734387171\n",
      "williams 0.6250934180805605\n",
      "tug 0.6174138520791097\n",
      "connects 0.6109611203633987\n",
      "\n",
      "***** Word2Vec *****\n",
      "vampire 0.4110491871833801\n",
      "imperative 0.37511640787124634\n",
      "keep 0.35370296239852905\n",
      "tear 0.3486976623535156\n",
      "sarcastic 0.3375144600868225\n",
      "glad 0.32664573192596436\n",
      "somehow 0.3167136013507843\n",
      "similarity 0.3162207305431366\n",
      "element 0.31535667181015015\n",
      "reunite 0.31460872292518616\n"
     ]
    }
   ],
   "source": [
    "compare_models('pleasant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input word: horrible\n",
      "\n",
      "***** SVD *****\n",
      "substandard 0.8575662926996017\n",
      "splatter 0.8255799390131067\n",
      "gunk 0.7439345253922862\n",
      "effect 0.6876224269840946\n",
      "university 0.6637234780406462\n",
      "junk 0.6442515511683667\n",
      "resulted 0.6390119351595118\n",
      "special 0.6338946697944025\n",
      "jeremy 0.6319104283926702\n",
      "script 0.6299355995607481\n",
      "\n",
      "***** Word2Vec *****\n",
      "way 0.6739404797554016\n",
      "know 0.6629747152328491\n",
      "would 0.6596165895462036\n",
      "go 0.6570124626159668\n",
      "wa 0.6563858985900879\n",
      "thing 0.6500155925750732\n",
      "work 0.6489824056625366\n",
      "time 0.6457241773605347\n",
      "show 0.640852689743042\n",
      "ha 0.6395604610443115\n"
     ]
    }
   ],
   "source": [
    "compare_models('horrible')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input word: loved\n",
      "\n",
      "***** SVD *****\n",
      "saw 0.7240355996286192\n",
      "amazing 0.7202507572225153\n",
      "one 0.7160288545049089\n",
      "great 0.7085565501348268\n",
      "treat 0.7064553825062345\n",
      "see 0.697794047790514\n",
      "kind 0.6932661382686476\n",
      "despite 0.6812781107393988\n",
      "disappointment 0.6801986666524373\n",
      "highly 0.6768343932707598\n",
      "\n",
      "***** Word2Vec *****\n",
      "joke 0.521105170249939\n",
      "ever 0.5123059749603271\n",
      "go 0.5119848847389221\n",
      "8 0.4847155213356018\n",
      "going 0.48196229338645935\n",
      "truly 0.47532257437705994\n",
      "human 0.4704762399196625\n",
      "know 0.46724292635917664\n",
      "work 0.46481919288635254\n",
      "case 0.45912566781044006\n"
     ]
    }
   ],
   "source": [
    "compare_models('loved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Judging from the results, I think that the word embeddings generated from SVD created more accurate results as the similar-words generated seem to make more sense than those produced from the word2vec model.\n",
    "\n",
    "In the pleasant example, results produced from the SVD word embeddings include delightful, help, unifying, heartstrings etc which are indeed words that are associated with the word pleasant. On the other hand, the results produced by the word2vec model include vampire, imperative, sarcastic, tear etc which are hardly close the meaning of pleasant.\n",
    "\n",
    "This same behaviour can be found in the other examples as well. Moreover, the cosine similarity scores for SVD word embedding results are generally higher than that of word2vec’s, with the exception of the zombie example. Hence I conclude that for this corpus, SVD word embedding is on average a better approach compared to using word2vec."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
